{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from globals.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import import_ipynb\n",
    "dir = Path('notebooks')\n",
    "sys.path.insert(0, str(dir.resolve()))\n",
    "import globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 5000\n",
    "NOISE = 0.005\n",
    "STEP_SIZE = 10\n",
    "STEPS = 100\n",
    "ALPHA = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = globals.CHANNELS, out_channels = 32, kernel_size = 5, stride = 2, padding = 1)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.swish = Swish()\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.shape[0]\n",
    "        x = self.conv2d_1(data)\n",
    "        x = self.swish(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.flatten(x)\n",
    "        x = nn.Linear(in_features = x.shape[1], out_features = batch_size)(x)\n",
    "        x = self.swish(x)\n",
    "        ebm_output = nn.Linear(in_features = batch_size, out_features = 1)(x)\n",
    "        return ebm_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langevin Sampling Function\n",
    "\n",
    "def generate_samples(model, inp_imgs, steps, step_size, noise):\n",
    "    imgs_per_step = []\n",
    "    for _ in range(steps): \n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        noise = torch.randn(size = inp_imgs.shape) * noise\n",
    "\n",
    "        inp_imgs.requires_grad = True\n",
    "\n",
    "        out_score = torch.mean(model(inp_imgs))\n",
    "\n",
    "        grads = torch.autograd.grad(outputs = out_score, \n",
    "                                    inputs = inp_imgs,\n",
    "                                    only_inputs = True)\n",
    "        \n",
    "        perturbation = step_size * torch.cat(grads) + noise\n",
    "        inp_imgs = inp_imgs.detach() + perturbation\n",
    "\n",
    "    return inp_imgs.detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, model):\n",
    "        super(Buffer, self).__init__()\n",
    "        self.model = model\n",
    "        self.examples = torch.concat([torch.rand(size = (1, globals.CHANNELS, globals.IMAGE_SIZE, globals.IMAGE_SIZE)) * 2 - 1\n",
    "                                      for _ in range(globals.BATCH_SIZE)],\n",
    "                                    dim = 0\n",
    "                                    )\n",
    "        \n",
    "\n",
    "    def sample_new_exmps(self, steps, step_size, noise, batch_size):\n",
    "        n_new = np.random.binomial(n = globals.BATCH_SIZE, p = 0.05)\n",
    "\n",
    "        rand_imgs = (torch.rand(size = (n_new, globals.CHANNELS, globals.IMAGE_SIZE, globals.IMAGE_SIZE)) * 2 - 1) \n",
    "        \n",
    "        old_imgs = torch.stack(\n",
    "            random.choices(population = self.examples, k = batch_size - n_new)\n",
    "        )\n",
    "\n",
    "\n",
    "        inp_imgs = torch.concat(tensors = [rand_imgs, old_imgs], dim = 0)\n",
    "\n",
    "        inp_imgs = generate_samples(model = self.model, inp_imgs = inp_imgs, steps = steps, step_size = step_size, noise = noise)\n",
    "        \n",
    "        new_img_examples = torch.concat(torch.split(inp_imgs, split_size_or_sections = batch_size, dim = 0), dim = 0)\n",
    "\n",
    "        self.examples = torch.concat(tensors = [new_img_examples, self.examples ], dim = 0)\n",
    "\n",
    "        self.examples = self.examples[:BUFFER_SIZE]\n",
    "\n",
    "        return inp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EBM, self).__init__()\n",
    "        self.model = Model()\n",
    "        self.buffer = Buffer(model = self.model)\n",
    "        self.opt = torch.optim.Adam(params = self.model.parameters(), lr = 0.0001, betas = (0.9, 0.999))\n",
    "\n",
    "    def forward(self, real_imgs):\n",
    "        batch_size = real_imgs.shape[0]\n",
    "        self.opt.zero_grad()\n",
    "        real_imgs += torch.normal(size = real_imgs.shape, mean = 0.0, std = NOISE)\n",
    "        fake_imgs = self.buffer.sample_new_exmps(steps = STEPS, step_size = STEP_SIZE, noise = NOISE, batch_size = batch_size)\n",
    "\n",
    "        real_scores = self.model(real_imgs)\n",
    "        fake_scores = self.model(fake_imgs)\n",
    "\n",
    "\n",
    "        cdiv_loss = torch.mean(fake_scores) - torch.mean(real_scores)\n",
    "\n",
    "        reg_loss = ALPHA * torch.mean(\n",
    "            real_scores ** 2 + fake_scores ** 2\n",
    "        )\n",
    "\n",
    "        loss = cdiv_loss + reg_loss\n",
    "        \n",
    "        print('real_scores is: {}'.format(torch.mean(real_scores)))\n",
    "        print('fake_scores is: {}'.format(torch.mean(fake_scores)))\n",
    "\n",
    "        print('cdiv_loss is: {}'.format(cdiv_loss))\n",
    "        print('reg_loss is: {}'.format(reg_loss))\n",
    "\n",
    "\n",
    "        print('Overall loss is: {}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
