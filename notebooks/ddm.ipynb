{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import import_ipynb\n",
    "dir = Path('notebooks')\n",
    "sys.path.insert(0, str(dir.resolve()))\n",
    "import globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMA = 0.999\n",
    "NOISE_EMBEDDING_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_cosine_diffusion_schedule(diffusion_time):\n",
    "    min_signal_rate = 0.02\n",
    "    max_signal_rate = 0.95\n",
    "    start_angle = torch.acos(torch.tensor(max_signal_rate))\n",
    "    end_angle = torch.acos(torch.tensor(min_signal_rate))\n",
    "\n",
    "    diffusion_angle = start_angle + diffusion_time * (end_angle - start_angle)\n",
    "\n",
    "    signal_rate = torch.cos(diffusion_angle)\n",
    "    noise_rate = torch.sin(diffusion_angle)\n",
    "\n",
    "    return signal_rate, noise_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x):\n",
    "    frequencies = torch.exp(input = torch.linspace(math.log(10), math.log(1000.0), NOISE_EMBEDDING_SIZE // 2))\n",
    "    angular_speed = 2.0 * math.pi * frequencies\n",
    "    embeddings = torch.concat(tensors = [torch.cos(angular_speed * x), torch.sin(angular_speed * x)], dim = 3)\n",
    "    embeddings = torch.permute(input = embeddings, dims = (0, 3, 1, 2))\n",
    "    return embeddings\n",
    "\n",
    "def Swish(x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[1]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = nn.Conv2d(in_channels = x.shape[1], out_channels = width, kernel_size = 1)(x)\n",
    "            \n",
    "        x = nn.BatchNorm2d(num_features = x.shape[1])(x)\n",
    "        x = nn.Conv2d(in_channels = x.shape[1], out_channels = width, kernel_size = 3, padding = 1)(x)\n",
    "        x = Swish(x)\n",
    "        x = nn.Conv2d(in_channels = x.shape[1], out_channels = width, kernel_size = 3, padding = 1)(x)\n",
    "        x = torch.add(input=x, other = residual)\n",
    "        return x\n",
    "    return apply\n",
    "    \n",
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = nn.AvgPool2d(kernel_size = 2)(x)\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "def UpBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "\n",
    "        x = nn.UpsamplingBilinear2d(scale_factor = 2)(x)\n",
    "\n",
    "        for _ in range(block_depth):\n",
    "            x = torch.concatenate(tensors = [x, skips.pop()], dim = 1)\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "    return apply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(U_Net, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = globals.CHANNELS, out_channels = 32, kernel_size = 1)\n",
    "        self.downblock_1 = DownBlock(width = 32, block_depth = 2)\n",
    "        self.downblock_2 = DownBlock(width = 64, block_depth = 2)\n",
    "        self.downblock_3 = DownBlock(width = 96, block_depth = 2)\n",
    "        self.residualblock_1 = ResidualBlock(width = 128)\n",
    "        self.residualblock_2 = ResidualBlock(width = 128)\n",
    "        self.upblock_1 = UpBlock(width = 96, block_depth = 2)\n",
    "        self.upblock_2 = UpBlock(width = 64, block_depth = 2)\n",
    "        self.upblock_3 = UpBlock(width = 32, block_depth = 2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 32, out_channels = 3, kernel_size = 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, noise_variance, noisy_image):\n",
    "\n",
    "        embedded_noise = sinusoidal_embedding(noise_variance)\n",
    "        upsampled_noise = nn.UpsamplingNearest2d(scale_factor = 64)(embedded_noise)\n",
    "\n",
    "        conv_noisy_image = self.conv2d_1(noisy_image)\n",
    "\n",
    "        x = torch.concat(tensors = [upsampled_noise, conv_noisy_image], dim = 1)\n",
    "\n",
    "        skips = []\n",
    "        x = self.downblock_1([x, skips])\n",
    "        x = self.downblock_2([x, skips])\n",
    "        x = self.downblock_3([x, skips])\n",
    "\n",
    "        x = self.residualblock_1(x)\n",
    "        x = self.residualblock_2(x)\n",
    "\n",
    "        x = self.upblock_1([x, skips])\n",
    "        x = self.upblock_2([x, skips])\n",
    "        x = self.upblock_3([x, skips])\n",
    "\n",
    "        pred_noise = self.conv2d_2(x)\n",
    "\n",
    "        return pred_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.network = U_Net()\n",
    "        self.ema_network = copy.deepcopy(self.network)\n",
    "        self.diffusion_schedule = offset_cosine_diffusion_schedule\n",
    "        self.opt_network = torch.optim.Adam(params = self.network.parameters(), lr = 0.0001)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.1)\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
    "        if training:\n",
    "            pred_noises = self.network(\n",
    "            noise_variance = noise_rates**2, noisy_image = noisy_images\n",
    "        )\n",
    "        else:\n",
    "            pred_noises = self.ema_network(\n",
    "            noise_variance = noise_rates**2, noisy_image = noisy_images\n",
    "        )\n",
    "        \n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "        current_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            diffusion_times = torch.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                current_images, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            current_images = (\n",
    "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "            )\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps, initial_noise=None):\n",
    "        if initial_noise is None:\n",
    "            initial_noise = torch.randn(\n",
    "                size=(num_images, globals.CHANNELS, globals.IMAGE_SIZE, globals.IMAGE_SIZE)\n",
    "            )\n",
    "        generated_images = self.reverse_diffusion(\n",
    "            initial_noise, diffusion_steps\n",
    "        )\n",
    "        generated_images = torch.mean(input = generated_images, dim = (0,1,2,3)) + generated_images * torch.var(input = generated_images, dim = (0,1,2,3))**0.5\n",
    "        return generated_images\n",
    "\n",
    "    def forward(self, images):\n",
    "        self.opt_network.zero_grad()\n",
    "        images = torch.tensor(nn.BatchNorm2d(images.shape[1])(images)).requires_grad_(True)\n",
    "\n",
    "        noises = torch.randn(size=(globals.BATCH_SIZE, globals.CHANNELS, globals.IMAGE_SIZE, globals.IMAGE_SIZE))\n",
    "\n",
    "        diffusion_times = torch.rand(\n",
    "            size=(globals.BATCH_SIZE, 1, 1, 1)\n",
    "        )\n",
    "        signal_rates, noise_rates = self.diffusion_schedule(diffusion_times)\n",
    "        \n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        pred_noises, _ = self.denoise(\n",
    "            noisy_images, noise_rates, signal_rates, training=True\n",
    "        )\n",
    "        noise_loss = torch.mean(nn.functional.mse_loss(noises, pred_noises)) \n",
    "\n",
    "        noise_loss.backward()\n",
    "        self.opt_network.step()\n",
    "\n",
    "        networks_state_dict = self.network.state_dict()\n",
    "        ema_networks_state_dict = self.ema_network.state_dict()\n",
    "\n",
    "\n",
    "        for (ema_name, ema_param), (_, param) in zip(\n",
    "            ema_networks_state_dict.items(), networks_state_dict.items()\n",
    "        ):\n",
    "            ema_networks_state_dict[ema_name] = EMA * ema_param + (1 - EMA) * param\n",
    "\n",
    "        print('Loss is  {}'.format(noise_loss))\n",
    "\n",
    "        return noise_loss\n",
    "    \n",
    "    def generate(self, images):\n",
    "        images = torch.tensor(nn.BatchNorm2d(images.shape[1])(images))\n",
    "        noises = torch.randn(size=(globals.BATCH_SIZE, globals.CHANNELS, globals.IMAGE_SIZE, globals.IMAGE_SIZE))\n",
    "        diffusion_times = torch.rand(\n",
    "            size=(globals.BATCH_SIZE, 1, 1, 1)\n",
    "        )\n",
    "        signal_rates, noise_rates = self.diffusion_schedule(diffusion_times)\n",
    "        \n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        pred_noises, _ = self.denoise(\n",
    "            noisy_images, noise_rates, signal_rates, training=False\n",
    "        )\n",
    "\n",
    "        noise_loss = torch.mean(nn.functional.mse_loss(noises, pred_noises))\n",
    "\n",
    "        print('Loss is  {}'.format(noise_loss))\n",
    "\n",
    "        return noise_loss\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
