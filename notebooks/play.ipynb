{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import sys\n",
    "import import_ipynb\n",
    "from pathlib import Path\n",
    "\n",
    "dir = Path('notebooks')\n",
    "sys.path.insert(0, str(dir.resolve()))\n",
    "import wgan\n",
    "import globals\n",
    "import ebm\n",
    "import ddm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize([globals.IMAGE_SIZE, globals.IMAGE_SIZE]), # Resizing images to 64 x 64\n",
    "    transforms.ToTensor(), # Converting images to tensors\n",
    "])\n",
    "traindataset = LoadDataset(root_dir = '../data/SARscope/train/', transform = transform)\n",
    "testdataset = LoadDataset(root_dir = '../data/SARscope/test/', transform = transform)\n",
    "validdataset = LoadDataset(root_dir = '../data/SARscope/valid/', transform = transform)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(dataset = traindataset, batch_size = globals.BATCH_SIZE, shuffle = True)\n",
    "testloader = DataLoader(dataset = testdataset, batch_size = globals.BATCH_SIZE, shuffle = True)\n",
    "validloader = DataLoader(dataset = validdataset, batch_size = globals.BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated_images(generator, epoch, num_images = 1):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(size = (num_images, globals.Z_DIM))\n",
    "        generated_images = generator(z)\n",
    "    generated_images = (generated_images + 1) / 2\n",
    "    generated_images = torch.squeeze(generated_images,0).numpy().transpose(1,2,0)\n",
    "    plt.imshow(generated_images)\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EBM = True\n",
    "DDM = False\n",
    "\n",
    "if EBM:\n",
    "    model = ebm.EBM()\n",
    "elif DDM:\n",
    "    model = ddm.DiffusionModel()\n",
    "else:\n",
    "    generator = wgan.Generator()\n",
    "    critic = wgan.Critic()\n",
    "    critic_optimizer = torch.optim.Adam(params = critic.parameters(), lr = 0.0001)\n",
    "    generator_optimizer = torch.optim.Adam(params = generator.parameters(), lr = 0.0001)\n",
    "    model = wgan.wgan(generator, critic, critic_optimizer, generator_optimizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch():\n",
    "\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        print('Batch number {}'.format(i))\n",
    "\n",
    "        loss = model(data)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "\n",
    "for epoch in range(globals.EPOCHS):\n",
    "    print('EPOCH {}'.format(epoch_number + 1))\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
